{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for experimenting and trying to improve model\n",
    "\n",
    "### Considerations to improve \n",
    "- Stopwords \n",
    "- Word2vec\n",
    "- bigger n-grams \n",
    "- bert? \n",
    "- max words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "import tensorflow as tf\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(filepath, padding = False):\n",
    "    train_vocab = {}\n",
    "    train = gzip.open(filepath)\n",
    "    counter1 = 0\n",
    "    counter2 = 0\n",
    "    counter3 = 0\n",
    "    counter = 0\n",
    "    if padding: \n",
    "        train_vocab['<PAD>'] = 0\n",
    "        counter2 += 1\n",
    "    no_reviewText = []\n",
    "    labels = {}\n",
    "    sentences = {}\n",
    "    tokenizer = TweetTokenizer()\n",
    "    for line in train:\n",
    "        counter1 +=1\n",
    "        #print(line)\n",
    "        if 'reviewText' in json.loads(line).keys():\n",
    "            a = json.loads(line)\n",
    "            sentences[counter3] = a['reviewText']\n",
    "            counter3 += 1\n",
    "            if a['sentiment'] == 'positive':\n",
    "                labels[counter] = 1\n",
    "            elif a['sentiment'] == 'negative': \n",
    "                labels[counter] = 0\n",
    "            counter +=1\n",
    "            for word in tokenizer.tokenize(json.loads(line)['reviewText']):\n",
    "                if word not in train_vocab.keys():\n",
    "                    train_vocab[word] = counter2\n",
    "                    counter2 += 1\n",
    "        else:\n",
    "            no_reviewText.append(counter1)\n",
    "    final_dict = {'line_count' : counter1,\n",
    "                 'review_count' : counter3,\n",
    "                 'vocab_size' : counter2,\n",
    "                 'no_text_reviews' : no_reviewText,\n",
    "                 'labels' : labels,\n",
    "                 'vocabulary' : train_vocab,\n",
    "                 'sentences' : sentences}\n",
    "    return final_dict\n",
    "\n",
    "def sen_vectorizer(filepath, cutoff = False): \n",
    "    vocab, index = {}, 1\n",
    "    data = gzip.open(filepath)\n",
    "    vocab['<PAD>'] = 0\n",
    "    counter1 = 0\n",
    "    counter2 = 0\n",
    "    counter3 = 0\n",
    "    counter = 0\n",
    "    no_reviewText = []\n",
    "    sentences = {}\n",
    "    tokenizer = TweetTokenizer()\n",
    "    labels = {}\n",
    "    for line in data:\n",
    "        counter1 +=1\n",
    "        #print(line)\n",
    "        if 'reviewText' in json.loads(line).keys():\n",
    "            a = json.loads(line)\n",
    "            b = tokenizer.tokenize(a['reviewText'])\n",
    "            if cutoff: \n",
    "                b = b[:cutoff]\n",
    "            sentences[counter3] = b\n",
    "            counter3 += 1\n",
    "            if a['sentiment'] == 'positive':\n",
    "                labels[counter] = 1\n",
    "            elif a['sentiment'] == 'negative': \n",
    "                labels[counter] = 0\n",
    "            counter +=1\n",
    "            for word in b:\n",
    "                if word not in vocab.keys():\n",
    "                    vocab[word] = index\n",
    "                    index += 1\n",
    "        else:\n",
    "            no_reviewText.append(counter1)\n",
    "    inverse_vocab = {index: token for token, index in vocab.items()}\n",
    "    final_dict = {'line_count' : counter1,\n",
    "                 'review_count' : counter3,\n",
    "                 'vocab_size' : counter2,\n",
    "                 'no_text_reviews' : no_reviewText,\n",
    "                 'labels' : labels,\n",
    "                 'vocabulary' : vocab,\n",
    "                 'sentences' : sentences,\n",
    "                 'inverse_vocab' : inverse_vocab}\n",
    "    return final_dict\n",
    "    \n",
    "def create_onehot(vocab, sentences, tokenzier):\n",
    "    # Create matrix\n",
    "    m1 = torch.zeros(len(sentences), len(vocab))\n",
    "    # Correct indices\n",
    "    for sen in range(len(sentences)): \n",
    "        for word in sentences[sen]: \n",
    "            if word in vocab.keys():\n",
    "                m1[sen, vocab[word]] = 1\n",
    "    return m1\n",
    "\n",
    "def create_batches(matrix, batch_size,labels): \n",
    "    num_batches = int(len(matrix)/batch_size)\n",
    "    feats_batches = matrix[:batch_size*num_batches].view(num_batches,batch_size, matrix.shape[1])\n",
    "    bingus = labels\n",
    "    num_batches = int(len(bingus)/batch_size)\n",
    "    label_batches = bingus[:batch_size*num_batches].view(num_batches,batch_size,1)\n",
    "    return feats_batches, label_batches\n",
    "\n",
    "paths = {'train':'../classification/music_reviews_train.json.gz',\n",
    "        'test':'../classification/music_reviews_test_masked.json.gz',\n",
    "        'dev' : '../classification/music_reviews_dev.json.gz'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up tokenizer, train and dev data\n",
    "tokenizer = TweetTokenizer()\n",
    "train_data = sen_vectorizer(paths['train'], cutoff = 100)\n",
    "train_matrix = create_onehot(train_data['vocabulary'], train_data['sentences'], TweetTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the official tensorflow way of making skip-grams\n",
    "# Can be tabbed out \n",
    "skip_grams = {}\n",
    "counter = 0\n",
    "for line in train_data['sentences']:\n",
    "    example_sequence = [train_data['vocabulary'][word] for word in train_data['sentences'][line]]\n",
    "    a = tf.keras.preprocessing.sequence.skipgrams(\n",
    "    example_sequence,\n",
    "    vocabulary_size=train_data['vocab_size'],\n",
    "    window_size=2,\n",
    "    negative_samples=0,\n",
    "    shuffle = False)\n",
    "    skip_grams[counter] = a\n",
    "    counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rob_skipgram(tokenized_sents, tokenizer, word2idx, window_size):\n",
    "    PAD = '<PAD>'\n",
    "    fullData = []\n",
    "    labels = []\n",
    "    for sent in tokenized_sents:\n",
    "        for tgtIdx in range(len(tokenized_sents[sent])):\n",
    "            labels.append(word2idx[tokenized_sents[sent][tgtIdx]])\n",
    "            dataLine = []\n",
    "            # backwards\n",
    "            for dist in reversed(range(1,window_size+1)):\n",
    "                srcIdx = tgtIdx - dist\n",
    "                if srcIdx < 0:\n",
    "                    dataLine.append(word2idx[PAD])\n",
    "                else:\n",
    "                    dataLine.append(word2idx[tokenized_sents[sent][srcIdx]])\n",
    "            # forwards\n",
    "            for dist in range(1,window_size+1):\n",
    "                srcIdx = tgtIdx + dist\n",
    "                if srcIdx >= len(tokenized_sents[sent]):\n",
    "                    dataLine.append(word2idx[PAD])\n",
    "                else:\n",
    "                    dataLine.append(word2idx[tokenized_sents[sent][srcIdx]])\n",
    "            fullData.append(dataLine)\n",
    "    return fullData, labels\n",
    "data, labels = rob_skipgram(train_data['sentences'], tokenizer, train_data['vocabulary'], 2)\n",
    "labels = torch.tensor(labels)\n",
    "data = torch.tensor(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3721204 3721204 99946\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([   1,    2,    3,  ...,   14, 5537,   20])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(data), len(labels), len(train_data['labels']))\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBOW(\n",
      "  (embeddings): Embedding(84746, 64)\n",
      "  (linear): Linear(in_features=64, out_features=84746, bias=True)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 64\n",
    "\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, emb_dim, vocab_dim):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_dim, emb_dim)\n",
    "        # note that embeddingsbag can also be used, then sum can be skipped in forward()\n",
    "        self.linear = nn.Linear(emb_dim, vocab_dim)\n",
    "        #self.activation_function = nn.Softmax(dim=0)\n",
    "        self.loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "    \n",
    "    def forward(self, inputs, gold):\n",
    "        embeds = self.embeddings(inputs)\n",
    "        out = torch.sum(embeds,dim=0)\n",
    "        out = self.linear(out)\n",
    "        out = self.loss_function(out, gold)\n",
    "        return out\n",
    "\n",
    "\n",
    "cbow = CBOW(embed_dim,len(train_data['vocabulary']))\n",
    "print(cbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0 0.0\n",
      "1000 9080.258647672832\n",
      "2000 18411.396780110896\n",
      "3000 27463.60071514873\n",
      "4000 36460.331681388896\n",
      "5000 45635.3055561739\n",
      "6000 54360.789154761704\n",
      "7000 63510.20990715292\n",
      "8000 72752.26343639311\n",
      "9000 82814.93785925326\n",
      "10000 92468.80426506815\n",
      "11000 101977.02085476997\n",
      "12000 111735.99196946505\n",
      "13000 121127.34042524057\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-b9e7620f755f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msub_l\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;31m# zero the parameter gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;31m# forward + backward + optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mzero_grad\u001b[0;34m(self, set_to_none)\u001b[0m\n\u001b[1;32m    214\u001b[0m                             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "feat_batches, label_batches = create_batches(data, 1000, labels)\n",
    "\n",
    "# compile and train the model\n",
    "optimizer = optim.SGD(cbow.parameters(), lr=0.001)\n",
    "counter = 0\n",
    "loop_nr = 1\n",
    "\n",
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    print(epoch)\n",
    "    for window, label in zip(feat_batches, label_batches):\n",
    "        for sub_w, sub_l in zip(window, label):\n",
    "            if counter % 1000 == 0:\n",
    "                print(counter, running_loss)\n",
    "            window = sub_w.view(-1, 1)\n",
    "            label = sub_l.view(1)\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            loss = cbow.forward(window, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            counter+=1\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '<PAD>',\n",
       " 1: 'So',\n",
       " 2: 'creative',\n",
       " 3: '!',\n",
       " 4: 'Love',\n",
       " 5: 'his',\n",
       " 6: 'music',\n",
       " 7: '-',\n",
       " 8: 'the',\n",
       " 9: 'words',\n",
       " 10: ',',\n",
       " 11: 'message',\n",
       " 12: 'Some',\n",
       " 13: 'of',\n",
       " 14: 'my',\n",
       " 15: 'favorite',\n",
       " 16: 'songs',\n",
       " 17: 'on',\n",
       " 18: 'this',\n",
       " 19: 'CD',\n",
       " 20: '.',\n",
       " 21: 'I',\n",
       " 22: 'should',\n",
       " 23: 'have',\n",
       " 24: 'bought',\n",
       " 25: 'it',\n",
       " 26: 'years',\n",
       " 27: 'ago',\n",
       " 28: 'This',\n",
       " 29: 'tape',\n",
       " 30: 'can',\n",
       " 31: 'hardly',\n",
       " 32: 'be',\n",
       " 33: 'understood',\n",
       " 34: 'and',\n",
       " 35: 'was',\n",
       " 36: 'listed',\n",
       " 37: 'for',\n",
       " 38: 'sale',\n",
       " 39: 'as',\n",
       " 40: '\"',\n",
       " 41: 'very',\n",
       " 42: 'good',\n",
       " 43: \"It's\",\n",
       " 44: 'VERY',\n",
       " 45: 'BAD',\n",
       " 46: 'Buy',\n",
       " 47: 'Do',\n",
       " 48: 'not',\n",
       " 49: 'buy',\n",
       " 50: 'MP3',\n",
       " 51: 'album',\n",
       " 52: 'Download',\n",
       " 53: 'is',\n",
       " 54: 'no',\n",
       " 55: 'longer',\n",
       " 56: 'available',\n",
       " 57: 'But',\n",
       " 58: 'you',\n",
       " 59: \"don't\",\n",
       " 60: 'find',\n",
       " 61: 'that',\n",
       " 62: 'out',\n",
       " 63: 'until',\n",
       " 64: 'after',\n",
       " 65: 'purchased',\n",
       " 66: 'love',\n",
       " 67: 'Dallas',\n",
       " 68: 'Holms',\n",
       " 69: 'voice',\n",
       " 70: 'Thank',\n",
       " 71: 'You',\n",
       " 72: 'will',\n",
       " 73: 'attending',\n",
       " 74: 'all',\n",
       " 75: 'concerts',\n",
       " 76: 'in',\n",
       " 77: 'heaven',\n",
       " 78: 'forever',\n",
       " 79: 'Great',\n",
       " 80: 'memories',\n",
       " 81: 'early',\n",
       " 82: 'Christ',\n",
       " 83: 'been',\n",
       " 84: 'listening',\n",
       " 85: 'to',\n",
       " 86: 'set',\n",
       " 87: 'ENTIRE',\n",
       " 88: 'life',\n",
       " 89: '(',\n",
       " 90: '30',\n",
       " 91: ')',\n",
       " 92: 'remember',\n",
       " 93: 'hearing',\n",
       " 94: 'repeat',\n",
       " 95: 'every',\n",
       " 96: 'night',\n",
       " 97: 'a',\n",
       " 98: 'toddler',\n",
       " 99: 'brought',\n",
       " 100: 'me',\n",
       " 101: 'comfort',\n",
       " 102: 'sleep',\n",
       " 103: 'always',\n",
       " 104: 'had',\n",
       " 105: 'some',\n",
       " 106: 'or',\n",
       " 107: 'fan',\n",
       " 108: 'rest',\n",
       " 109: 'Well',\n",
       " 110: 'worth',\n",
       " 111: 'relaxation',\n",
       " 112: 'Talbot',\n",
       " 113: 'inspiring',\n",
       " 114: 'since',\n",
       " 115: 'gift',\n",
       " 116: 'just',\n",
       " 117: 'myself',\n",
       " 118: 'soon',\n",
       " 119: 'Most',\n",
       " 120: 'unique',\n",
       " 121: 'sound',\n",
       " 122: 'Nothing',\n",
       " 123: 'even',\n",
       " 124: 'similar',\n",
       " 125: 'among',\n",
       " 126: 'praise',\n",
       " 127: 'choices',\n",
       " 128: 'Truly',\n",
       " 129: 'real',\n",
       " 130: 'worshipful',\n",
       " 131: 'am',\n",
       " 132: 'referring',\n",
       " 133: 'only',\n",
       " 134: 'older',\n",
       " 135: 'The',\n",
       " 136: 'Lords',\n",
       " 137: 'supper',\n",
       " 138: 'part',\n",
       " 139: 'extending',\n",
       " 140: 'track',\n",
       " 141: '9',\n",
       " 142: 'so',\n",
       " 143: 'item',\n",
       " 144: 'ordered',\n",
       " 145: 'arrived',\n",
       " 146: 'timely',\n",
       " 147: 'manner',\n",
       " 148: 'quite',\n",
       " 149: 'impressed',\n",
       " 150: 'quality',\n",
       " 151: 'cd',\n",
       " 152: '/',\n",
       " 153: 'i',\n",
       " 154: 'fantastic',\n",
       " 155: 'satisfied',\n",
       " 156: 'If',\n",
       " 157: 'your',\n",
       " 158: 'preferences',\n",
       " 159: 'religious',\n",
       " 160: 'fall',\n",
       " 161: 'half',\n",
       " 162: 'way',\n",
       " 163: 'between',\n",
       " 164: 'old',\n",
       " 165: 'church',\n",
       " 166: 'chants',\n",
       " 167: 'modern',\n",
       " 168: 'artist',\n",
       " 169: 'listen',\n",
       " 170: 'first',\n",
       " 171: 'heard',\n",
       " 172: \"Lord's\",\n",
       " 173: 'Supper',\n",
       " 174: 'vinyl',\n",
       " 175: 'back',\n",
       " 176: '80',\n",
       " 177: \"'\",\n",
       " 178: 's',\n",
       " 179: 'trying',\n",
       " 180: 'get',\n",
       " 181: 'copy',\n",
       " 182: 'ever',\n",
       " 183: 'come',\n",
       " 184: 'conclusion',\n",
       " 185: 'print',\n",
       " 186: 'but',\n",
       " 187: 'thrilled',\n",
       " 188: 'still',\n",
       " 189: 'through',\n",
       " 190: 'Amazon',\n",
       " 191: 'Although',\n",
       " 192: 'recorded',\n",
       " 193: 'late',\n",
       " 194: '70',\n",
       " 195: \"lyric's\",\n",
       " 196: 'taken',\n",
       " 197: 'from',\n",
       " 198: 'liturgies',\n",
       " 199: 'dating',\n",
       " 200: 'several',\n",
       " 201: 'centuries',\n",
       " 202: 'today',\n",
       " 203: 'has',\n",
       " 204: 'wonderful',\n",
       " 205: 'ability',\n",
       " 206: 'move',\n",
       " 207: 'one',\n",
       " 208: 'deeply',\n",
       " 209: 'profoundly',\n",
       " 210: 'recording',\n",
       " 211: 'flow',\n",
       " 212: 'almost',\n",
       " 213: 'making',\n",
       " 214: 'contains',\n",
       " 215: 'two',\n",
       " 216: 'John',\n",
       " 217: 'Michael',\n",
       " 218: \"Talbot's\",\n",
       " 219: 'earliest',\n",
       " 220: 'releases',\n",
       " 221: 'third',\n",
       " 222: 'Christian',\n",
       " 223: '79',\n",
       " 224: 'which',\n",
       " 225: 'Catholic',\n",
       " 226: 'Be',\n",
       " 227: 'Exalted',\n",
       " 228: 'fifteenth',\n",
       " 229: 'released',\n",
       " 230: '86',\n",
       " 231: 'Containing',\n",
       " 232: '22',\n",
       " 233: 'tracks',\n",
       " 234: 'compilation',\n",
       " 235: 'worship',\n",
       " 236: 'many',\n",
       " 237: 'essential',\n",
       " 238: 'made',\n",
       " 239: 'name',\n",
       " 240: 'household',\n",
       " 241: 'word',\n",
       " 242: 'most',\n",
       " 243: 'homes',\n",
       " 244: 'probably',\n",
       " 245: 'signature',\n",
       " 246: 'release',\n",
       " 247: 'may',\n",
       " 248: 'give',\n",
       " 249: 'five',\n",
       " 250: 'stars',\n",
       " 251: 'overtures',\n",
       " 252: 'few',\n",
       " 253: 'other',\n",
       " 254: 'stray',\n",
       " 255: 'too',\n",
       " 256: 'far',\n",
       " 257: 'particular',\n",
       " 258: 'taste',\n",
       " 259: 'quiet',\n",
       " 260: 'are',\n",
       " 261: 'musically',\n",
       " 262: 'ears',\n",
       " 263: 'rock',\n",
       " 264: 'n',\n",
       " 265: 'roll',\n",
       " 266: 'lover',\n",
       " 267: 'And',\n",
       " 268: 'others',\n",
       " 269: 'impressive',\n",
       " 270: 'because',\n",
       " 271: \"Card's\",\n",
       " 272: 'across',\n",
       " 273: 'heart',\n",
       " 274: 'felt',\n",
       " 275: 'theological',\n",
       " 276: 'thought',\n",
       " 277: 'winning',\n",
       " 278: 'alignment',\n",
       " 279: 'One',\n",
       " 280: 'albums',\n",
       " 281: 'could',\n",
       " 282: 'solely',\n",
       " 283: 'also',\n",
       " 284: 'recommend',\n",
       " 285: 'Present',\n",
       " 286: 'Reality',\n",
       " 287: 'about',\n",
       " 288: \"I've\",\n",
       " 289: 'loved',\n",
       " 290: 'never',\n",
       " 291: 'afford',\n",
       " 292: 'now',\n",
       " 293: 'Card',\n",
       " 294: 'brings',\n",
       " 295: 'ol',\n",
       " 296: 'days',\n",
       " 297: 'Holm',\n",
       " 298: 'His',\n",
       " 299: 'truly',\n",
       " 300: 'great',\n",
       " 301: 'scratches',\n",
       " 302: 'U',\n",
       " 303: 'feel',\n",
       " 304: 'our',\n",
       " 305: 'God',\n",
       " 306: 'Gives',\n",
       " 307: 'us',\n",
       " 308: 'hope',\n",
       " 309: 'strength',\n",
       " 310: 'These',\n",
       " 311: \"America's\",\n",
       " 312: 'Greatest',\n",
       " 313: 'Old-time',\n",
       " 314: 'Gospel',\n",
       " 315: 'they',\n",
       " 316: 'presented',\n",
       " 317: 'an',\n",
       " 318: 'abomination',\n",
       " 319: 'especially',\n",
       " 320: 'Old-Time',\n",
       " 321: 'soul',\n",
       " 322: 'sung',\n",
       " 323: 'Neither',\n",
       " 324: 'nor',\n",
       " 325: 'found',\n",
       " 326: 'these',\n",
       " 327: 'arrangements',\n",
       " 328: 'would',\n",
       " 329: 'compare',\n",
       " 330: 'redition',\n",
       " 331: 'classics',\n",
       " 332: 'fast',\n",
       " 333: 'food',\n",
       " 334: \"wouldn't\",\n",
       " 335: 'really',\n",
       " 336: 'fair',\n",
       " 337: 'nutritional',\n",
       " 338: 'value',\n",
       " 339: 'plastic',\n",
       " 340: 'burned',\n",
       " 341: 'with',\n",
       " 342: 'Few',\n",
       " 343: 'turn',\n",
       " 344: 'Scripture',\n",
       " 345: 'into',\n",
       " 346: 'song',\n",
       " 347: 'well',\n",
       " 348: 'lyrics',\n",
       " 349: 'cover',\n",
       " 350: 'book',\n",
       " 351: 'Bible',\n",
       " 352: 'new',\n",
       " 353: 'focuses',\n",
       " 354: 'gospel',\n",
       " 355: 'Luke',\n",
       " 356: 'collection',\n",
       " 357: 'associated',\n",
       " 358: ':',\n",
       " 359: 'Amazement',\n",
       " 360: 'four',\n",
       " 361: 'commentaries',\n",
       " 362: 'gospels',\n",
       " 363: 'Biblical',\n",
       " 364: 'Imagination',\n",
       " 365: 'Series',\n",
       " 366: 'A',\n",
       " 367: 'commentary',\n",
       " 368: 'remaining',\n",
       " 369: 'each',\n",
       " 370: 'next',\n",
       " 371: 'three',\n",
       " 372: 'currently',\n",
       " 373: 'hosting',\n",
       " 374: 'seminars',\n",
       " 375: 'country',\n",
       " 376: 'teaching',\n",
       " 377: 'how',\n",
       " 378: 'engage',\n",
       " 379: 'at',\n",
       " 380: 'level',\n",
       " 381: 'seeking',\n",
       " 382: 'bring',\n",
       " 383: 'actual',\n",
       " 384: 'Essence',\n",
       " 385: 'Runes',\n",
       " 386: 'culture',\n",
       " 387: 'discovered',\n",
       " 388: 'them',\n",
       " 389: 'then',\n",
       " 390: 'Live',\n",
       " 391: 'breath',\n",
       " 392: 'power',\n",
       " 393: 'Odin',\n",
       " 394: 'chanted',\n",
       " 395: 'around',\n",
       " 396: 'recordings',\n",
       " 397: 'show',\n",
       " 398: 'written',\n",
       " 399: 'used',\n",
       " 400: 'raise',\n",
       " 401: 'own',\n",
       " 402: 'vibration',\n",
       " 403: 'match',\n",
       " 404: 'Shamans',\n",
       " 405: 'live',\n",
       " 406: 'youth',\n",
       " 407: 'Taize',\n",
       " 408: 'meets',\n",
       " 409: 'stringent',\n",
       " 410: 'requirements',\n",
       " 411: 'beauty',\n",
       " 412: 'reverence',\n",
       " 413: 'sophisticated',\n",
       " 414: 'MA',\n",
       " 415: 'theology',\n",
       " 416: 'embodies',\n",
       " 417: 'best',\n",
       " 418: 'liberal',\n",
       " 419: 'catholic',\n",
       " 420: 'ecumenical',\n",
       " 421: 'tradition',\n",
       " 422: 'It',\n",
       " 423: 'perfect',\n",
       " 424: 'private',\n",
       " 425: 'simpler',\n",
       " 426: 'pieces',\n",
       " 427: 'utilized',\n",
       " 428: 'protestant',\n",
       " 429: 'liturgy',\n",
       " 430: \"I'm\",\n",
       " 431: 'SAT',\n",
       " 432: 'tutor',\n",
       " 433: 'if',\n",
       " 434: 'stand',\n",
       " 435: 'Ms',\n",
       " 436: \"Mazer's\",\n",
       " 437: 'annoying',\n",
       " 438: 'stories',\n",
       " 439: 'her',\n",
       " 440: 'learn',\n",
       " 441: 'approach',\n",
       " 442: 'effective',\n",
       " 443: 'For',\n",
       " 444: 'teenagers',\n",
       " 445: 'Mazer',\n",
       " 446: 'much',\n",
       " 447: 'to--my',\n",
       " 448: 'son',\n",
       " 449: 'refuses',\n",
       " 450: 'However',\n",
       " 451: 'listened',\n",
       " 452: \"CD's\",\n",
       " 453: 'learned',\n",
       " 454: 'remembered',\n",
       " 455: 'suggest',\n",
       " 456: 'borrow',\n",
       " 457: 'friend',\n",
       " 458: 'try',\n",
       " 459: 'before',\n",
       " 460: 'more',\n",
       " 461: 'than',\n",
       " 462: 'Princeton',\n",
       " 463: 'Reviews',\n",
       " 464: 'Word',\n",
       " 465: 'Smart',\n",
       " 466: 'certainly',\n",
       " 467: 'expand',\n",
       " 468: 'vocabulary',\n",
       " 469: 'Normally',\n",
       " 470: 'associate',\n",
       " 471: 'idea',\n",
       " 472: 'increasing',\n",
       " 473: 'lifting',\n",
       " 474: 'yourself',\n",
       " 475: 'higher',\n",
       " 476: 'communication',\n",
       " 477: 'bit',\n",
       " 478: 'dignified',\n",
       " 479: 'want',\n",
       " 480: 'successful',\n",
       " 481: 'people',\n",
       " 482: 'world',\n",
       " 483: 'know',\n",
       " 484: 'what',\n",
       " 485: 'taking',\n",
       " 486: 'Or',\n",
       " 487: 'high',\n",
       " 488: 'school',\n",
       " 489: 'run',\n",
       " 490: 'away',\n",
       " 491: 'hormones',\n",
       " 492: 'impress',\n",
       " 493: 'friends',\n",
       " 494: 'bathroom',\n",
       " 495: 'go',\n",
       " 496: 'In',\n",
       " 497: 'minutes',\n",
       " 498: \"you'll\",\n",
       " 499: 'agree',\n",
       " 500: '100',\n",
       " 501: '%',\n",
       " 502: 'reviewer',\n",
       " 503: 'Leanne',\n",
       " 504: 'Way',\n",
       " 505: 'Too',\n",
       " 506: 'Scary',\n",
       " 507: 'Vocabulary',\n",
       " 508: 'excited',\n",
       " 509: 'product',\n",
       " 510: 'appeared',\n",
       " 511: 'fun',\n",
       " 512: 'hilarious',\n",
       " 513: 'family-friendly',\n",
       " 514: \"children's\",\n",
       " 515: 'started',\n",
       " 516: 'playing',\n",
       " 517: 'car',\n",
       " 518: 'daughter',\n",
       " 519: '12',\n",
       " 520: 'were',\n",
       " 521: 'difficult',\n",
       " 522: 'material',\n",
       " 523: 'got',\n",
       " 524: 'embarrassing',\n",
       " 525: 'seemed',\n",
       " 526: 'related',\n",
       " 527: \"author's\",\n",
       " 528: 'sex',\n",
       " 529: 'marketed',\n",
       " 530: 'prep',\n",
       " 531: 'any',\n",
       " 532: 'graduate',\n",
       " 533: 'tests',\n",
       " 534: 'study',\n",
       " 535: 'GRE',\n",
       " 536: 'use',\n",
       " 537: 'aid',\n",
       " 538: 'ten',\n",
       " 539: 'anecdotes',\n",
       " 540: 'proved',\n",
       " 541: 'irritating',\n",
       " 542: 'condescending',\n",
       " 543: 'CDs',\n",
       " 544: 'helpful',\n",
       " 545: 'enjoyable',\n",
       " 546: 'young',\n",
       " 547: 'students',\n",
       " 548: 'studying',\n",
       " 549: 'ACT',\n",
       " 550: 'adults',\n",
       " 551: 'college',\n",
       " 552: 'stay',\n",
       " 553: 'wish',\n",
       " 554: 'when',\n",
       " 555: 'preparing',\n",
       " 556: 'SATs',\n",
       " 557: 'Renee',\n",
       " 558: 'clever',\n",
       " 559: 'poems',\n",
       " 560: 'catchy',\n",
       " 561: 'often',\n",
       " 562: 'easy',\n",
       " 563: 'think',\n",
       " 564: 'she',\n",
       " 565: 'created',\n",
       " 566: 'brilliant',\n",
       " 567: 'opening',\n",
       " 568: 'doors',\n",
       " 569: 'whole',\n",
       " 570: 'technique',\n",
       " 571: 'Forget',\n",
       " 572: 'boring',\n",
       " 573: 'classes',\n",
       " 574: 'we',\n",
       " 575: 'take',\n",
       " 576: 'Just',\n",
       " 577: 'pick',\n",
       " 578: 'up',\n",
       " 579: 'watch',\n",
       " 580: 'score',\n",
       " 581: 'Highly',\n",
       " 582: 'recommended',\n",
       " 583: 'When',\n",
       " 584: 'ordering',\n",
       " 585: 'interactive',\n",
       " 586: 'worked',\n",
       " 587: 'computer',\n",
       " 588: '&',\n",
       " 589: 'audio',\n",
       " 590: 'visual',\n",
       " 591: 'opportunity',\n",
       " 592: 'test',\n",
       " 593: 'offered',\n",
       " 594: 'unless',\n",
       " 595: 'format',\n",
       " 596: 'changed',\n",
       " 597: 'offer',\n",
       " 598: 'student',\n",
       " 599: 'experience',\n",
       " 600: 'Anyone',\n",
       " 601: 'woman',\n",
       " 602: 'girl',\n",
       " 603: '?',\n",
       " 604: 'produce',\n",
       " 605: '...',\n",
       " 606: 'Its',\n",
       " 607: 'impossible',\n",
       " 608: 'distracted',\n",
       " 609: 'by',\n",
       " 610: 'persons',\n",
       " 611: 'buyer',\n",
       " 612: 'beware',\n",
       " 613: 'contemporary',\n",
       " 614: 'ministry',\n",
       " 615: 'genre',\n",
       " 616: 'Twila',\n",
       " 617: 'Paris',\n",
       " 618: 'Rebecca',\n",
       " 619: 'St',\n",
       " 620: 'James',\n",
       " 621: 'Micheal',\n",
       " 622: 'Smith',\n",
       " 623: 'Integrity',\n",
       " 624: 'Hillsong',\n",
       " 625: 'everything',\n",
       " 626: 'played',\n",
       " 627: 'sang',\n",
       " 628: 'single',\n",
       " 629: 'moving',\n",
       " 630: 'Danielle',\n",
       " 631: 'Rose',\n",
       " 632: 'reveal',\n",
       " 633: 'faith',\n",
       " 634: 'understanding',\n",
       " 635: 'deep',\n",
       " 636: 'She',\n",
       " 637: 'special',\n",
       " 638: '..',\n",
       " 639: 'touch',\n",
       " 640: 'him',\n",
       " 641: \"Sheryl's\",\n",
       " 642: 'watched',\n",
       " 643: 'gestation',\n",
       " 644: 'contributed',\n",
       " 645: 'highly',\n",
       " 646: 'gems',\n",
       " 647: 'ready',\n",
       " 648: 'opened',\n",
       " 649: 'let',\n",
       " 650: 'overflow',\n",
       " 651: 'send',\n",
       " 652: 'someone',\n",
       " 653: 'else',\n",
       " 654: 'time',\n",
       " 655: 'holidays',\n",
       " 656: 'Let',\n",
       " 657: 'their',\n",
       " 658: 'hearts',\n",
       " 659: 'change',\n",
       " 660: 'sticker',\n",
       " 661: 'says',\n",
       " 662: 'ages',\n",
       " 663: 'reason',\n",
       " 664: 'story',\n",
       " 665: 'simply',\n",
       " 666: 'explained',\n",
       " 667: 'myths',\n",
       " 668: 'grand',\n",
       " 669: 'adventures',\n",
       " 670: 'filled',\n",
       " 671: 'lessons',\n",
       " 672: 'freeway',\n",
       " 673: 'during',\n",
       " 674: 'rush',\n",
       " 675: 'hour',\n",
       " 676: 'did',\n",
       " 677: 'mind',\n",
       " 678: 'sitting',\n",
       " 679: 'bumper',\n",
       " 680: 'regarding',\n",
       " 681: 'Midas',\n",
       " 682: 'sets',\n",
       " 683: 'pace',\n",
       " 684: 'Not',\n",
       " 685: 'told',\n",
       " 686: 'captivating',\n",
       " 687: 'moral',\n",
       " 688: 'end',\n",
       " 689: 'Also',\n",
       " 690: 'appreciated',\n",
       " 691: 'referral',\n",
       " 692: 'library',\n",
       " 693: 'aware',\n",
       " 694: 'medium',\n",
       " 695: 'witted',\n",
       " 696: 'piece',\n",
       " 697: 'containing',\n",
       " 698: 'jokes',\n",
       " 699: 'conservative',\n",
       " 700: 'politics',\n",
       " 701: 'bible',\n",
       " 702: 'As',\n",
       " 703: 'dentistry',\n",
       " 704: 'who',\n",
       " 705: 'share',\n",
       " 706: 'relationship',\n",
       " 707: 'living',\n",
       " 708: 'Non-Christians',\n",
       " 709: 'walk',\n",
       " 710: 'laughs',\n",
       " 711: 'none',\n",
       " 712: 'wiser',\n",
       " 713: 'completely',\n",
       " 714: 'fake',\n",
       " 715: 'engineer',\n",
       " 716: 'Intrigued',\n",
       " 717: 'promotional',\n",
       " 718: 'video',\n",
       " 719: 'saw',\n",
       " 720: 'Facebook',\n",
       " 721: 'claim',\n",
       " 722: 'crickets',\n",
       " 723: 'singing',\n",
       " 724: 'melody',\n",
       " 725: 'We',\n",
       " 726: 'slowed',\n",
       " 727: 'down',\n",
       " 728: 'version',\n",
       " 729: 'took',\n",
       " 730: 'original',\n",
       " 731: 'make',\n",
       " 732: 'comparison',\n",
       " 733: 'Guess',\n",
       " 734: 'There',\n",
       " 735: 'objection',\n",
       " 736: 'melodies',\n",
       " 737: 'thats',\n",
       " 738: 'interesting',\n",
       " 739: 'thing',\n",
       " 740: 'do',\n",
       " 741: 'Claiming',\n",
       " 742: 'god',\n",
       " 743: 'given',\n",
       " 744: 'natural',\n",
       " 745: 'Beautiful',\n",
       " 746: 'sublime',\n",
       " 747: 'peaceful',\n",
       " 748: 'Dumbfounded',\n",
       " 749: 'Gid',\n",
       " 750: 'handiwork',\n",
       " 751: 'Hoax',\n",
       " 752: 'tuber',\n",
       " 753: 'recreated',\n",
       " 754: \"doesn't\",\n",
       " 755: 'like',\n",
       " 756: 'http://youtu.be/qnMmYejDIZ0',\n",
       " 757: 'lot',\n",
       " 758: 'better',\n",
       " 759: 'Would',\n",
       " 760: 'short',\n",
       " 761: 'paid',\n",
       " 762: ':(',\n",
       " 763: 'disappointed',\n",
       " 764: 'why',\n",
       " 765: \"didn't\",\n",
       " 766: 'least',\n",
       " 767: 'loop',\n",
       " 768: 'something',\n",
       " 769: 'Thankyou',\n",
       " 770: 'enjoyed',\n",
       " 771: 'remarkable',\n",
       " 772: 'Will',\n",
       " 773: 'Hope',\n",
       " 774: 'comments',\n",
       " 775: 'purports',\n",
       " 776: 'there',\n",
       " 777: 'cricket',\n",
       " 778: 'choirs',\n",
       " 779: 'touring',\n",
       " 780: 'selling',\n",
       " 781: 'concert',\n",
       " 782: 'halls',\n",
       " 783: 'everywhere',\n",
       " 784: \"it's\",\n",
       " 785: 'represented',\n",
       " 786: 'phenomenon',\n",
       " 787: 'time-stretched',\n",
       " 788: 'sounds',\n",
       " 789: 'relative',\n",
       " 790: 'lifespan',\n",
       " 791: 'That',\n",
       " 792: 'NOT',\n",
       " 793: 'seemingly',\n",
       " 794: 'emerges',\n",
       " 795: 'angelic',\n",
       " 796: 'harmonies',\n",
       " 797: 'fabrication',\n",
       " 798: 'toy',\n",
       " 799: 'sampling',\n",
       " 800: 'keyboard',\n",
       " 801: 'doubt',\n",
       " 802: 'Twisted',\n",
       " 803: 'Hair',\n",
       " 804: 'Robbie',\n",
       " 805: 'Robertson',\n",
       " 806: 'hear',\n",
       " 807: 'SOME',\n",
       " 808: 'noises',\n",
       " 809: 'See',\n",
       " 810: '[',\n",
       " 811: ']',\n",
       " 812: 'clearly',\n",
       " 813: 'mix',\n",
       " 814: 'varying',\n",
       " 815: 'degrees',\n",
       " 816: 'human',\n",
       " 817: 'singers',\n",
       " 818: 'musicians',\n",
       " 819: 'Mr',\n",
       " 820: 'Wilson',\n",
       " 821: 'representing',\n",
       " 822: 'Personally',\n",
       " 823: 'distracting',\n",
       " 824: 'HOAX',\n",
       " 825: 'sounding',\n",
       " 826: 'record',\n",
       " 827: 'sadly',\n",
       " 828: 'definite',\n",
       " 829: 'hoax',\n",
       " 830: 'say',\n",
       " 831: 'chirps',\n",
       " 832: 'cannot',\n",
       " 833: 'boys',\n",
       " 834: 'choir',\n",
       " 835: 'Bear',\n",
       " 836: 'here',\n",
       " 837: '1',\n",
       " 838: 'Speed',\n",
       " 839: 'references',\n",
       " 840: 'online',\n",
       " 841: 'said',\n",
       " 842: '8x',\n",
       " 843: 'slower',\n",
       " 844: 'point',\n",
       " 845: \"we'd\",\n",
       " 846: 'chirpiness',\n",
       " 847: 'individual',\n",
       " 848: 'beats',\n",
       " 849: 'within',\n",
       " 850: 'chirp',\n",
       " 851: 'gaps',\n",
       " 852: 'albeit',\n",
       " 853: 'inaudibly',\n",
       " 854: 'low',\n",
       " 855: 'frequency',\n",
       " 856: 'reference',\n",
       " 857: 'proportional',\n",
       " 858: 'Jim',\n",
       " 859: 'write',\n",
       " 860: 'He',\n",
       " 861: 'stolen',\n",
       " 862: 'opera',\n",
       " 863: 'singer',\n",
       " 864: 'Bonnie',\n",
       " 865: 'Jo',\n",
       " 866: 'Hunt',\n",
       " 867: 'Here',\n",
       " 868: 'last',\n",
       " 869: 'Music',\n",
       " 870: 'Native',\n",
       " 871: 'Americans',\n",
       " 872: 'called',\n",
       " 873: 'infringement',\n",
       " 874: 'copyright',\n",
       " 875: 'wanted',\n",
       " 876: '2nd',\n",
       " 877: 'background',\n",
       " 878: 'loud',\n",
       " 879: 'finally',\n",
       " 880: 'Raven',\n",
       " 881: \"God's\",\n",
       " 882: 'Cricket',\n",
       " 883: 'Chorus',\n",
       " 884: 'Order',\n",
       " 885: 'magnitude',\n",
       " 886: \"can't\",\n",
       " 887: 'believe',\n",
       " 888: \"there's\",\n",
       " 889: 'Show',\n",
       " 890: 'Mercy',\n",
       " 891: 'Thumbs',\n",
       " 892: 'sure',\n",
       " 893: 'came',\n",
       " 894: 'ridiculous',\n",
       " 895: 'listing',\n",
       " 896: 'DDAMN',\n",
       " 897: 'SS13',\n",
       " 898: 'Really',\n",
       " 899: 'Pretty',\n",
       " 900: 'sad',\n",
       " 901: 'omissions',\n",
       " 902: 'New',\n",
       " 903: 'Slayer',\n",
       " 904: 'fans',\n",
       " 905: 'Start',\n",
       " 906: 'Reign',\n",
       " 907: 'Blood',\n",
       " 908: 'entire',\n",
       " 909: 'catalog',\n",
       " 910: 'waste',\n",
       " 911: 'Very',\n",
       " 912: 'Good',\n",
       " 913: 'Helpful',\n",
       " 914: 'Joyce',\n",
       " 915: 'Always',\n",
       " 916: 'Has',\n",
       " 917: 'An',\n",
       " 918: 'Anointed',\n",
       " 919: 'Say',\n",
       " 920: 'All',\n",
       " 921: \"Joyce's\",\n",
       " 922: 'Books',\n",
       " 923: 'DVDs',\n",
       " 924: 'GVB',\n",
       " 925: 'tight',\n",
       " 926: 'rich',\n",
       " 927: 'chills',\n",
       " 928: 'spine',\n",
       " 929: 'Carry',\n",
       " 930: 'America',\n",
       " 931: 'Like',\n",
       " 932: 'Teach',\n",
       " 933: 'World',\n",
       " 934: ';',\n",
       " 935: 'lift',\n",
       " 936: 'spirit',\n",
       " 937: 'Having',\n",
       " 938: 'groupe',\n",
       " 939: 'base',\n",
       " 940: 'action',\n",
       " 941: 'entirely',\n",
       " 942: 'expected',\n",
       " 943: 'Low',\n",
       " 944: 'Down',\n",
       " 945: 'Chariot',\n",
       " 946: 'Gentle',\n",
       " 947: 'Shepard',\n",
       " 948: 'recomend',\n",
       " 949: 'absolutely',\n",
       " 950: 'question',\n",
       " 951: 'guys',\n",
       " 952: 'sing',\n",
       " 953: 'scream',\n",
       " 954: 'nearly',\n",
       " 955: 'gotten',\n",
       " 956: 'cappella',\n",
       " 957: 'lately',\n",
       " 958: 'looking',\n",
       " 959: 'forward',\n",
       " 960: 'excellent',\n",
       " 961: \"wasn't\",\n",
       " 962: 'darn',\n",
       " 963: 'pass',\n",
       " 964: 'greatest',\n",
       " 965: 'produced',\n",
       " 966: 'Gaither',\n",
       " 967: 'Vocal',\n",
       " 968: 'Band',\n",
       " 969: 'Bill',\n",
       " 970: 'Guy',\n",
       " 971: 'David',\n",
       " 972: 'Russ',\n",
       " 973: 'shown',\n",
       " 974: 'effort',\n",
       " 975: 'tremendously',\n",
       " 976: 'talented',\n",
       " 977: 'versatile',\n",
       " 978: 'style',\n",
       " 979: 'honored',\n",
       " 980: 'such',\n",
       " 981: 'talent',\n",
       " 982: 'choice',\n",
       " 983: 'arrangement',\n",
       " 984: 'blend',\n",
       " 985: 'voices',\n",
       " 986: 'touching',\n",
       " 987: 'over',\n",
       " 988: 'inspirational',\n",
       " 989: 'beautiful',\n",
       " 990: 'Disc',\n",
       " 991: '2',\n",
       " 992: 'load',\n",
       " 993: 'player',\n",
       " 994: 'disc',\n",
       " 995: 'tim',\n",
       " 996: 'reed',\n",
       " 997: 'Classic',\n",
       " 998: 'stop',\n",
       " 999: 'praising',\n",
       " ...}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['inverse_vocab']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds = cbow.state_dict()['embeddings.weight']\n",
    "outFile = open('embeds.txt', 'w')\n",
    "outFile.write(str(len(train_data['inverse_vocab'])) + ' ' + str(embed_dim) + '\\n')\n",
    "for word, embed in zip(train_data['inverse_vocab'], embeds):\n",
    "    outFile.write(word + ' ' + ' '.join([str(x) for x in embed.tolist()]) + '\\n')\n",
    "outFile.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
